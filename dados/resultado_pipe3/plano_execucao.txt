== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Sort [ano#153 ASC NULLS FIRST, funcionario#152 ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning(ano#153 ASC NULLS FIRST, funcionario#152 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=732]
      +- Project [ano#153, funcionario#152, total_unidades_vendidas#140, total_unidades_ano#136, round(((total_unidades_vendidas#140 / total_unidades_ano#136) * 100.0), 2) AS proporcional_func_ano#134]
         +- BroadcastHashJoin [ano#153], [ano#18], Inner, BuildRight, false
            :- HashAggregate(keys=[ano#153, funcionario#152], functions=[sum(unidades_vendidas#155)])
            :  +- Exchange hashpartitioning(ano#153, funcionario#152, 200), ENSURE_REQUIREMENTS, [plan_id=719]
            :     +- HashAggregate(keys=[ano#153, funcionario#152], functions=[partial_sum(unidades_vendidas#155)])
            :        +- Filter isnotnull(ano#153)
            :           +- FileScan csv [funcionario#152,ano#153,unidades_vendidas#155] Batched: false, DataFilters: [isnotnull(ano#153)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/data/dados1_cap03.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ano)], ReadSchema: struct<funcionario:string,ano:int,unidades_vendidas:double>
            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=728]
               +- HashAggregate(keys=[ano#18], functions=[sum(total_unidades_vendidas#135)])
                  +- Exchange hashpartitioning(ano#18, 200), ENSURE_REQUIREMENTS, [plan_id=725]
                     +- HashAggregate(keys=[ano#18], functions=[partial_sum(total_unidades_vendidas#135)])
                        +- HashAggregate(keys=[ano#18, funcionario#17], functions=[sum(unidades_vendidas#20)])
                           +- Exchange hashpartitioning(ano#18, funcionario#17, 200), ENSURE_REQUIREMENTS, [plan_id=721]
                              +- HashAggregate(keys=[ano#18, funcionario#17], functions=[partial_sum(unidades_vendidas#20)])
                                 +- Filter isnotnull(ano#18)
                                    +- FileScan csv [funcionario#17,ano#18,unidades_vendidas#20] Batched: false, DataFilters: [isnotnull(ano#18)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/data/dados1_cap03.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ano)], ReadSchema: struct<funcionario:string,ano:int,unidades_vendidas:double>


